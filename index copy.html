<!DOCTYPE html>

<html lang="en">
<head>
<meta charset="utf-8"/>
<meta content="IE=edge" http-equiv="X-UA-Compatible"/>
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
<title>Data Governance Driven ETL</title>
<!-- Bootstrap -->
<link href="css/bootstrap.min.css" rel="stylesheet"/>
<link href="css/bootstrap-theme.css" rel="stylesheet"/>
<link href="css/swiper.min.css" rel="stylesheet"/>
<style>
/*!
 * Author: Akash Bhadange
 */

 

 body {
  background: linear-gradient(315deg, #c31e14 0%, #4F54AC 100%)
  font-family: 'Open Sans', sans-serif;
  margin: 0;
  padding: 0;
  background-color: #f4f4f4;
  color: #333;
 
  font-size: 16px;
  color: #222;
  font-weight: normal;
  letter-spacing: 0.03em;
}
a {
  color: #ff7b7b;
  -webkit-transition: all 0.2s;
  -moz-transition: all 0.2s;
  transition: all 0.2s;
}
section {
  width: 100%;
}
a:hover, a:focus, a:active {
  text-decoration: none;
  outline: none;
}
ul {
  margin: 0;
  padding: 0;
}
ul li {
  list-style: none;
}
img {
  max-width: 100%;
}
.purple {
  color: #7447ae;
}
.pink {
  color: #ff2a75;
}
.blue {
  color: #1253c3;
}
.yellow{
  color: #f87428;
}
.b{
  color: #0291ff;
}
 
.text-left {
  color: white;
}

h1, h2, h3, h4, h5, h6 {
  font-family: 'Your Desired Font', sans-serif;
  /* Add your desired font name in place of 'Your Desired Font' */
}

.testimonial-box {
  position: relative;
}

.video-wrapper {
  position: relative;
  padding-bottom: 56.25%; /* 16:9 aspect ratio (adjust as needed) */
  height: 0;
  overflow: hidden;
}

.video-wrapper video {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}


 

h3, h4, h1, p {
  font-family: 'Montserrat', sans-serif;
}
p {
  font-size: small;
}






header.hero {
  position: relative;
  background: linear-gradient(45deg, #8686ba 0%, #f4ebec 100%)
  padding-bottom: 0px;
  overflow: hidden; /* Hide the overflow of the pseudo-element */
}

header.hero::before {
  content: "";
  position: absolute;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
   
  -webkit-background-size: cover;
  -moz-background-size: cover;
  -o-background-size: cover;
  background-size: cover;

  /* Apply blur filter to the pseudo-element only */
  -webkit-filter: blur(5px);
  filter: blur(3px);

  /* Ensure the pseudo-element is behind the content */
  z-index: -1;
}












 

header.hero .hero-text {
  
  margin-top: 20%;
  margin-left: 0%;
  margin-bottom: 20%;

}
header.hero .row {
  position: relative;
}
a.menu {
  position: absolute;
  right: 0;
  top: 3em;
}
a.menu img {
  max-width: 40px;
}
a.menu:hover {
  transform: translateX(-5px);
  -webkit-transform: translateX(-5px);
}
header.hero h1 {
  font-family: 'Montserrat', sans-serif;
  font-size: 4.5em;
  font-weight: 600;
  line-height: 1.4em;
}
header.hero h1 span,
header.hero h3 {
  font-weight: 300;
}
header.hero .btn {
  margin: 3em 0;
}

ul.social-links li {
  display: inline-block;
}
ul.social-links li a {
  padding: 5px;
  opacity: 0.6;
}
ul.social-links li.label {
  font-size: 20px;
  opacity: 0.6;
  font-weight: 400;
}
ul.social-links li a:hover {
  opacity: 1;
}
ul.social-links li a img {
  max-height: 30px;
}




 


/* Case study */
.case-study .col-md-12 {
  transform: translateY(-80px);
  -webkit-transform: translateY(-80px);
  padding: 5em 3em;
  background-color: #fff;
  box-shadow: 0 0 100px rgba(0, 0, 0, 0.15);
}
h4.sub-heading {
  text-transform: uppercase;
  font-family: 'Montserrat', sans-serif;
  font-weight: 400;
  color: #4e4c58;
}
h1.heading {
  font-family: 'Montserrat', sans-serif;
  font-weight: 400;
  color: #4e4c58;
  font-size: 3em;
  position: relative;
  padding-bottom: 10px;
}
h1.heading span {
  font-weight: 600;
}
h1.heading:after {
  content: '';
  display: block;
  position: absolute;
  bottom: -10px;
  left: 0;
  height: 3px;
  width: 100px;
}
h1.heading.purple:after {
  background-color: #7447ae;
}
h1.heading.pink:after {
  background-color: #ff2a75;
}


.swiper-container {
  width: 100%;
  height: 100%;
  padding: 4em 1em;
}
.swiper-slide {
  font-size: 18px;
  background: #fff;
}

.client-box {
  background-color: #fff;
  box-shadow: 0 0 20px rgba(0, 0, 0, 0.03);
  padding: 3.5em 2em;
  border-radius: 5px;
  border: 1px solid #eee;
  text-align: center;
  -webkit-transition: all 0.2s;
  -moz-transition: all 0.2s;
  transition: all 0.2s;
}
.client-box .client-logo {
  max-height: 120px;
}
.client-box h3.title {
  color: #4e4c58;
  margin-bottom: 2em;
}
.client-box p.tag {
  color: #7447ae;
  font-family: 'Montserrat', sans-serif;
  margin-bottom: 1em;
}
.client-box a {
  color: #333;
  font-style: italic;
  opacity: 0.5;
}
.client-box.swiper-slide-next {
  -ms-transform: scale(1.15); /* IE 9 */
  -webkit-transform: scale(1.15); /* Safari */
  transform: scale(1.15);
  box-shadow: 0 0 40px rgba(0, 0, 0, 0.1);
  z-index: 2;
}
.swiper-button-next, .swiper-button-prev {
  width: 80px;
  height: 80px;
  background-size: 80px 80px;
  box-shadow: 0 0 40px rgba(0, 0, 0, 0.05);
  border-radius: 5em;
  -webkit-transition: all 0.2s;
  -moz-transition: all 0.2s;
  transition: all 0.2s;
}
.swiper-button-next:hover,
.swiper-button-prev:hover {
  box-shadow: 0 0 40px rgba(0, 0, 0, 0.1);
}
.swiper-button-next, .swiper-container-rtl .swiper-button-prev {
  background-image: url(../assets/arrow-right.png);
  right: -40px;
}
.swiper-button-prev, .swiper-container-rtl .swiper-button-next {
  background-image: url(../assets/arrow-left.png);
  left: -40px;
}


/* Testimonial */
.testimonial-box {
  padding: 2em;
  color: #fff;
  border-radius: 5px;
  font-weight: 300;
  min-height: 400px;
}
.testimonial-box h1 {
  font-family: 'Montserrat', sans-serif;
  font-weight: 400;
  margin-top: 0;
  margin-bottom: 1em;
}
.testimonial-box p.name {
  font-family: 'Montserrat', sans-serif;
  font-weight: 400;
  font-size: 1.2em;
  margin-bottom: 0;
}
.testimonial-box p.designation {
  font-family: 'Montserrat', sans-serif;
  margin-bottom: 0;
}
.testimonial-box.yellow {
  background-color: #ffbe61;
}
.testimonial-box.purple {
  background-color: #7b32d5;
}
.testimonial-box.blue {
  background-color: #37a7ff;
}
.testimonial-box.pink {
  background-color: #ff662a;
}


/* Statistics */
.stats {
  margin: 4em 0;
}
.stat-box {
  padding: 4em;
  margin-top: 4em;
  margin-bottom: 4em;
}
.stat-box h1 {
  font-family: 'Montserrat', sans-serif;
  font-weight: 600;
  font-size: 5.6em;
}
.stat-box h3 {
  font-family: 'Montserrat', sans-serif;
  font-weight: 600;
  color: #4e4c58;
  font-size: 2em;
}


/* Contact Banner */
.contact-banner {
  background: url(../assets/bg-pat.png) #664fee repeat;
  color: #fff;
  padding: 6em;
}
.contact-banner h1 {
  font-family: 'Montserrat', sans-serif;
  font-weight: 400;
  font-size: 3em;
  margin-top: 0;
  margin-bottom: 1em;
}


/* Footer */
.footer {
  background-color: #37324b;
  padding: 5em;
}
.footer p{
  color: #ffffff;
}
.footer h1 {
  font-family: 'Montserrat', sans-serif;
  font-weight: 600;
  font-size: 3em;
  color: #a3a1ac;
  margin-bottom: 2em;
}
.footer h1 span {
  color: #fff;
  font-weight: 300;
}
.footer h1:after {
  content: '';
  width: 80px;
  height: 4px;
  background-color: #37a7ff;
  display: block;
  position: absolute;
  left: 50%;
  transform: translate(-40px, 10px);
}
.footer ul.social-links li a {
  padding-left: 12px;
  padding-right: 12px;
}
.footer ul.social-links li a img {
  max-height: 40px;
}

/* Sub-footer */
.sub-footer {
  background-color: #322e45;
  padding: 2em;
  color: #a3a1ac;
  font-family: 'Montserrat', sans-serif;
  font-weight: 300;
}
.sub-footer p {
  margin: 0;
  font-size: 1.2em;
}
.sub-footer p a {
  color: #fff;
}
.sub-footer p a:hover {
  opacity: 0.8;
}


.footer {
  text-align: center; /* Center-aligns the text of all children elements */
  display: flex;
  justify-content: center; /* Horizontally centers the content */
  align-items: center; /* Vertically centers the content */
  flex-direction: column; /* Aligns children elements in a column */
}

.social-links {
  list-style-type: none; /* Removes bullet points from the list */
  padding: 0; /* Removes any padding from the list */
}

.social-links li {
  display: inline-block; /* Displays list items side by side */
  margin: 0 10px; /* Gives a little space between each list item */
}


/* Main navigation */
nav {
  height: 40px;
  width: 100%;
  position: absolute;
  right: 0;
  top: 3em;
  cursor: pointer;
  transition: .25s ease-in-out;
}
#menu-toggle {
  width: 40px;
  clear: both;
  float: right;
}
#menu-toggle span.line {
  display: block;
  height: 4px;
  width: 40px;
  background-color: #fff;
  transition: .25s ease-in-out;
}
#menu-toggle .hamburger span.line {
  margin-bottom: 0.45em;
  clear: both;
  float: right;
}
#menu-toggle .hamburger span.line:nth-child(1) {
  width: 30px;
  transition-delay: .25s;
}
#menu-toggle .hamburger span.line:nth-child(2) {
  transition-delay: .325s;
}
#menu-toggle .hamburger span.line:nth-child(3) {
  width: 20px;
  transition-delay: .475s;
}

#menu-toggle .cross span.line {
  width: 0px;
  position: absolute;
  top: 10px;
  right: 0;
}
#menu-toggle .cross span.line:nth-child(1) {
  transform: rotate(45deg);
  transition-delay: .0s;
}
#menu-toggle .cross span.line:nth-child(2) {
  transform: rotate(-45deg);
  transition-delay: .25s;
}

#menu-toggle.open .hamburger span.line {
  width: 0px;
}
#menu-toggle.open #hamburger span:nth-child(1) {
  transition-delay: 0s;
}
#menu-toggle.open #hamburger span:nth-child(2) {
  transition-delay: .125s;
}
#menu-toggle.open #hamburger span:nth-child(3) {
  transition-delay: .25s;
}

#menu-toggle.open .cross span.line:nth-child(1) {
  width: 40px;
  transition-delay: 1s;
}
#menu-toggle.open .cross span.line:nth-child(2) {
  width: 40px;
  transition-delay: 1.25s;
}

ul.main-nav {
  padding-right: 20px;
  display: flex;
  justify-content: flex-end;
  opacity: 0;
  visibility: hidden;
  transition: .25s all;
}
ul.main-nav li {
  display: inline-block;
}
ul.main-nav li a {
  color: #fff;
  font-family: 'Montserrat', sans-serif;
  font-size: 14px;
  text-transform: uppercase;
  padding: 5px 15px;
}
ul.main-nav li a:hover {
  color: #ffbe61;
}
ul.main-nav.show-it {
  opacity: 1;
  visibility: visible;
  transition-delay: .25s;
}


.headd{

  font-family: 'Open Sans', sans-serif;

}










/* Let's optimise it for mobile devices */

@media (max-width: 720px){
  .btn-lg {
    padding: .8em 3em;
    font-size: .8em;
  }
  h1.heading {
    font-size: 2em;
  }
  header.hero {
    text-align: center;
    padding-bottom: 0;
  }
  header.hero h1 {
    font-size: 2em;
    margin-top: 1.5em;
  }
  header.hero h3 {
    font-size: 1.2em;
    line-height: 1.6em;
  }
  ul.social-links li.label {
    display: block;
    font-size: 16px;
    margin-bottom: 1em;
  }
  ul.social-links li a {
    opacity: 1;
  }
  .testimonial {
    padding-top: 3rem;
  }
  .case-study .col-md-12 {
    padding-left: 15px;
    padding-right: 15px;
  }
  .case-study .col-md-12 {
    transform: translateY(0px);
    -webkit-transform: translateY(0px);
  }

  /*Swiper classes*/
  .client-box.swiper-slide-next {
    -ms-transform: scale(1);
    -webkit-transform: scale(1);
    transform: scale(1);
    box-shadow: none;
  }
  .swiper-button-prev,
  .swiper-button-next {
    display: none;
  }
  .swiper-container {
    padding: 4em 0em;
  }
  .client-box {
    padding: 1em;
    text-align: center;
  }
  .stat-box {
    padding: 1em;
    margin-top: 1em;
    margin-bottom: 1em;
  }
  .stat-box h1 {
    font-size: 3.6em;
  }
  .stat-box h3 {
    font-size: 1.4em;
  }
  .contact-banner {
    padding: 2em;
  }
  .contact-banner h1 {
    font-size: 2em;
  }
  .footer {
    padding: 1em 0em;
  }
}








/* Mobile view adjustments */
@media (max-width: 768px) {
    body {
        font-size: 14px;
    }

    h1, h2, h3, h4, h5, h6 {
        font-size: inherit;
    }

    img {
        max-width: 100%;
        height: auto;
    }
}

/* Desktop view adjustments */
@media (min-width: 992px) {
    body {
        margin-left: 10%;
        margin-right: 10%;
    }
}

/* Fix for "Pipeline 2 Overview" and "Compliance with UK GDPR" sections */
div[id^='pipeline'], div[id^='gdpr'] {
    display: block;
}
div[id^='pipeline'] img, div[id^='gdpr'] img {
    display: block;
    width: 100%;
    height: auto;
}
div[id^='pipeline'] p, div[id^='gdpr'] p {
    display: block;
    width: 100%;
}
</style>
<!-- Google Font -->
<link href="https://fonts.googleapis.com/css?family=Montserrat:300,400,500,600,700|Open+Sans:300,400,700" rel="stylesheet"/>
<!-- HTML5 shim and Respond.js for IE8 support of HTML5 elements and media queries -->
<!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
<!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
      <![endif]-->
<style>
 

        h2 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
            margin-top: 20px;
        }

        li {
            margin-bottom: 10px;
        }

        strong {
            color: #5a5a5f;
        }


      
          table {
              width: 100%;
              border-collapse: collapse;
              margin: 20px 0;
              font-family: Arial, sans-serif;
          }
          th, td {
              border: 1px solid #ddd;
              padding: 8px 12px;
              text-align: left;
          }
          th {
              background-color: #f2f2f2;
              font-weight: bold;
          }
          tr:nth-child(even) {
              background-color: #f5f5f5;
          }
          tr:hover {
              background-color: #ddd;
          }
          caption {
              font-size: 24px;
              font-weight: bold;
              margin-bottom: 15px;
          }





           /* Container for the images */
        .image-container {
          display: flex;
          flex-wrap: wrap;
          justify-content: space-around;
          padding: 20px;
      }

      /* Styling each individual image box */
      .img-box {
          margin: 20px;
          border: 1px solid #ddd;
          padding: 10px;
          background-color: #fff;
          box-shadow: 2px 2px 12px #aaa;
      }

      img {
          max-width: 100%;
          height: auto;
      }

      h3 {
          text-align: center;



      }
      .box-container {
        display: flex;
        justify-content: space-between; /* This will space the boxes evenly */
        padding: 20px;
        flex-wrap: nowrap; /* This ensures the boxes won't wrap to the next line */
    }

    /* Styling each individual box */
    .box {
        flex: 1; /* Each box will take an equal portion of the width */
        border: 1px solid #ddd;
        background-color: #fff;
        box-shadow: 2px 2px 12px #aaa;
        text-align: center;
        padding: 20px;
        margin: 0 10px; /* Add some space between the boxes */
    }

    strong {
      color: #000000;
  }
  em {
      color: #3498DB;
  }
  .condition-box {
      border: 2px solid #E74C3C;
      padding: 15px;
      margin-bottom: 20px;
  }
  .true-branch, .false-branch {
      background-color: #ECF0F1;
      padding: 10px;
      margin: 10px 0;
      border-radius: 5px;
  }


 
    .content-wrapper {
        margin-left: 50px;
    }

    .on-premises-heading {
      color: #000000; /* Neutral color for on-premises, as it's not an Azure product */
      font-weight: 500;
      font-size: 15px;
  }

  .adls-heading {
      color: #0078D4; /* Azure blue color, often associated with many Azure services */
      font-weight: 500;
      font-size: 15px;
  }

  .adf-heading {
      color: #03617b; /* A shade of Azure blue associated with Data Factory */
      font-weight: 500;
      font-size: 15px;
  }

  .databricks-heading {
      color: #f01f4d; /* Databricks pink color */
      font-weight: 500;
      font-size: 15px;
  }

  .sql-server-heading {
      color: #00449c; /* SQL Server red color */
      font-weight: 500;
      font-size: 15px;
  }


  .text-paragraph p {
       color: #03617b; /* A shade of Azure blue associated with Data Factory */
      font-weight: 500;
      font-size: 15px;

  }

  .ss {
    color: #000000; /* SQL Server red color */
    font-weight: 500;
    font-size: 15px;
}

.pp p {
  color: #000000; /* SQL Server red color */
  font-weight: 500;
  font-size: 10px;


}

.he   {
  font-family: 'Poppins', sans-serif;
  color: #000000; /* SQL Server red color */
  font-weight: 500;
  font-size: 28px;


}
 

  






      </style>
<link href="https://unpkg.com/aos@2.3.1/dist/aos.css" rel="stylesheet"/>
<script src="https://unpkg.com/aos@2.3.1/dist/aos.js"></script>
<link href="https://fonts.googleapis.com/css?family=Montserrat:300,400,500,600,700|Open+Sans:300,400,700" rel="stylesheet"/></head>
<body>
<header class="hero" data-aos="fade-right" data-aos-easing="ease-in-sine" data-aos-offset="300">
<div class="container" data-aos="zoom-out-up">
<div class="row" data-aos="zoom-out-up">
<div class="col-md-6 col-md-offset-6 col-xs-12">
<!-- <a href="#" class="menu"><img src="assets/menu.png"></a> -->
<div class="hero-text">
<h1 class="heading purple"><span class="purple"> Data modeling on</span> National Student survey 2023  </h1>
<p class="pp"> 
  In this blog post, I delve into the process of creating a dashboard by leveraging the STAR model NSS data. The focus is on guiding readers through the steps necessary to efficiently transform the STAR model National Student Survey (NSS) data into a dynamic and informative dashboard. This exploration aims to provide valuable insights and practical approaches for effectively visualizing data, enhancing the understanding and interpretation of NSS outcomes.</p>
<a style="color: black;"><p class="pp"><h5>By Siddharth Chikalkar</h5></p></a>
<h5></h5>
<br/><br/><br/><br/><br/><br/><br/><br/><br/><br/>
</div>
</div>
</div>
</div>
</header>
<section class="case-study">
<div class="container">
<div class="row">
<div class="col-md-12">
<h4 class="sub-heading" data-aos="fade-up" id="targetSection">Understading NSS data</h4>
<h1 class="heading purple"><span class="purple">Types of provider-level view</span> in NSS  </h1>
<!-- Swiper -->
<div class="container" style="font-family: Arial, sans-serif; margin-bottom: 20px;"><br/>
<h5>Description of Data Sources</h5>
<p style="text-align: justify;">
  The project involves the Extract, Load, Transform (ELT) process of consolidating and analyzing data from three distinctive data sources, each providing insight into stock market trends and company financial performance. The sources encompass real-time data feeds such as APIs and Azure functions, as well as static datasets from an on-premises source.
  </p>
<p>
      Our policies set the general standards and guidelines, ensuring data's quality, privacy, security, and appropriate retention. The rules delineate the operational specifics, from access control to data encryption and change management. Lastly, the classifications offer a detailed breakdown of data types, emphasizing their significance and the recommended protective measures for each. Together, these components manifest our commitment to upholding data integrity, privacy, and security throughout all stages of the ETL process.
  </p>
</div>






<table>
  <tr>
  <th>Provider level</th>
  <th>Dataset provide</th>
  </tr>
  <tr>
  <td><h5>Teaching</h5></td>
  <td>
  <p>2023 NSS results by teaching provider (full-time) (XLSX, 97.4 MB).</p>
  <p>2023 NSS results by teaching provider (part-time) (XLSX, 6.5 MB)</p>
  <p>2023 NSS results by teaching provider (apprenticeship) (XLSX, 7.1 MB)</p>
  </td>
  </tr>
  <tr>
  <td><h5>Registering</h5></td>
  <td>
  <p> </p>
  <p>2023 NSS results by registering provider (full-time) (XLSX, 95.4 MB)</p>
  <p>2023 NSS results by registering provider (part-time) (XLSX, 6.3 MB)</p>
  <p>2023 NSS results by registering provider (apprenticeship) (XLSX, 7.1 MB)</p>


  </td>
  </tr>
   
  
  </table>


 

<h1 class="heading pink"> <br/>Let's understading it on teaching provider level</h1>
<img alt="Data Lineage Overview" data-aos="zoom-in-down" src="assets\NSS teaching files.png"/>
<p>Data Lineage Overview</p>
<p>
      The core architecture of this project's data lineage commences with data sources housed on-premises. This foundational data undergoes a series of orchestrated transformations and movements, ensuring optimal utility and compliance throughout its lifecycle.
  </p>
<p class="on-premises-heading">On-Premises Data Source</p>
<p>
      Our starting point is the on-premises data source. Rich with diverse datasets, this source serves as the initial reservoir, feeding data into subsequent stages of the lineage.
  </p>
<p class="adls-heading">Azure Data Lake Storage (ADLS)</p>
<p>
      Acting as a critical intermediary, the ADLS stages the data ingested from the on-premises source. This pivotal stage aids in storing vast amounts of structured and unstructured data, offering high-speed and secure data analytics.
  </p>
<p class="adf-heading">Azure Data Factory (ADF)</p>
<p>
      Superimposed above, ADF takes charge of the orchestration. With its suite of tools like parameterization, Lookup activities, stored procedures, metadata activities, and more, ADF ensures that data flows seamlessly and efficiently through its intended pipeline. This involves activities like condition checks using 'If' conditions, iterative processing with 'ForEach' loops, and data management operations like deleting through the 'Delete' activity.
  </p>
<p class="databricks-heading">Azure Databricks</p>
<p>
      Post-staging, Azure Databricks steps in for data transformation. It accesses the staged data to conduct complex transformations, ensuring the data is structured, cleaned, and enriched for downstream consumption. Furthermore, the project's design ensures that only personnel with higher authority perform transformations on sensitive data, such as PII.
  </p>
<p class="sql-server-heading">SQL Server</p>
<p>
      Finally, the lineage includes the SQL Server, which plays a dual role. It not only acts as a repository for the transformed data but also houses the critical Metadata DB. This database meticulously documents metadata, capturing the nuanced details of every process and transformation undergone in the pipeline, reinforcing the project's commitment to transparency and traceability.
  </p>
<p>
      In summary, this data lineage is a testament to the project's dedication to efficient, transparent, and compliant data management and processing. Every stage is carefully designed to ensure data's integrity, security, and utility are preserved and enhanced.
  </p>
<div class="container" style="font-family: Arial, sans-serif; margin-bottom: 20px;">
<div class="img-box">
<h2 class="heading-primary">Implementing Data Governance</h2>
<p class="text-paragraph">
            In the digital age, aligning with stringent data governance protocols is not just ideal, but essential. The table below offers a comprehensive look into how our project's activities align with core governance policies. Each entry correlates a governance policy with its related activity, underscoring our commitment to data integrity, security, and privacy throughout the project's duration. We welcome stakeholders to dive deep into this synthesis, amplifying our dedication to transparent data practices.
        </p>
<table border="1" cellpadding="5" cellspacing="0">
<thead>
<tr>
<th>Data Governance Policy</th>
<th>Associated Project Activity</th>
</tr>
</thead>
<tbody>
<tr>
<td>Data Privacy Policy</td>
<td>
                        - Masking PII Data On-Prem<br/>
                        - Tiered Access in Databricks for PII data
                    </td>
</tr>
<tr>
<td>Data Security Policy</td>
<td>
                        - Leveraging Private Endpoints for secured connectivity between on-premises and Azure<br/>
                        - Limiting Databricks transformation access to privileged personnel
                    </td>
</tr>
<tr>
<td>Data Retention Policy</td>
<td>
                        - Purging data from ADLS post-transfer to Azure Blob Storage<br/>
                        - Differentiating PII from business data for tailored retention
                    </td>
</tr>
<tr>
<td>Data Quality Policy</td>
<td>
                        - Selective data extraction for ADLS staging<br/>
                        - Conducting Data Quality Checks (DQC) to ensure data consistency during the Databricks transformation phase
                    </td>
</tr>
<tr>
<td>Metadata Collection Policy</td>
<td>
                        - Metadata documentation via SQL Server stored procedures<br/>
                        - Detailed logging of data movement and transformations
                    </td>
</tr>
</tbody>
</table>
</div>
<br/><br/>
<div class="container">
<h2 class="heading-primary">UK GDPR Adherence: Pseudonymization Explained</h2>
<p class="text-paragraph">
            Pseudonymization is a technique of replacing sensitive data with synthetic identifiers, bridging the gap between data protection and compliance with regulations such as the UK GDPR.
        </p>
<h4 class="ss">Detailed Breakdown</h4>
<p class="text-paragraph">
            Here we are taking care of Data Minimization and Data pseudonymization, By selecting only specific columns ETL process incorporates with <span style="color: black;"><b>Article 5(1)(c)</b></span> of minimization of PII data.
            The SQL code provided enacts pseudonymization on select columns from the <code>customers</code> table. Let's dissect the steps:
        </p>
<ol>
<h4 class="ss">Targeted Columns</h4>
<p class="text-paragraph">
                The columns chosen for pseudonymization from the <code>customers</code> table include: CustomerID, CompanyName, ContactName, and ContactTitle. These columns house potential PII, mandating their pseudonymization for GDPR alignment. Additionally, sales analysis requires location-specific sales data. As PII protection mandates securing data that can pinpoint individuals, only data fulfilling this criterion is pseudonymized.
            </p>
<p class="ss">Copy Data Activity</p>
<p class="text-paragraph">This step, named <em>'Staging Area tables'</em>, forms a connection with the on-site database. Using the mentioned schema and table details, the corresponding datasets are retrieved.</p>
<li>
<p class="text-paragraph"><b>City:</b> This consideration is intricate. Major cities usually pose a low identification risk. However, smaller locales can expose individuals when combined with other data points. If the dataset contains many unique, smaller cities, maintaining them can heighten identification risks.</p>
</li>
<li>
<p class="text-paragraph"><b>Country:</b> Being a general data point, it's often not deemed as PII in isolation. Retaining country data is generally acceptable.</p>
</li>
<p class="text-paragraph">
              The SQL procedure leverages the <code>DENSE_RANK()</code> function to assign a unique rank to distinct column values. This rank, prefixed with a string, forms the pseudonym, replacing the original value.
          </p>
</ol>
</div>
</div>
<div class="image-container">
<div class="img-box">
<img alt="ETL Pipeline 1" data-aos="zoom-in-down" src="assets\method.png"/>
</div>
<ul>
<h4 class="ss">Compliance with UK GDPR</h4>
<p>Under the UK GDPR, organizations are encouraged to implement data protection principles, including  Data minimization and pseudonymisation.</p>
<li> <h4 class="ss">Data Minimization</h4> <p> By selecting only the relevant columns and applying pseudonymization only to PII info, we ensure that the least amount of personal data necessary is processed.</p></li>
<li> <h4 class="ss">Pseudonymisation</h4> <p>  The transformation of the original data to pseudonyms ensures that the data, in its pseudonymized form, cannot be directly attributed to a specific data subject without the use of additional information. This enhances data protection and allows for more secure data analytics and processing.</p></li>
<p>In conclusion, by applying these techniques, we are adhering to the best practices prescribed by the UK GDPR, ensuring that our data handling processes are both secure and compliant.</p>
</ul>
<h4>before and  after Data pseudonymization</h4>
<img alt="Data linage Overview" data-aos="zoom-in-down" src="assets\psy.png"/>
<br/><p>In above picture of pseudonymization data you can see some Contact Titles are respective and because of <code>DENSE_RANK()</code> function we can able to make same pseudonymization for that respective values. That will let us filter down the data in during visualization. </p>
<section id="introduction">
<h2>Introduction to the Azure cloud ETL Pipelines</h2>
<p>
                            At the forefront of data engineering and orchestration, this project's success hinges on its sophisticated ETL pipelines, meticulously engineered using an array of modern tools and practices. Serving as the spine of the entire data integration process, these pipelines deftly navigate through the intricacies of on-premises systems and cloud solutions, notably Azure Data Lake, all while remaining firmly anchored in GDPR compliance.
                        </p>
<p>
                            Throughout the pipelines, advanced techniques and components have been employed. These include but are not limited to: parametrization in Azure Data Factory (ADF), extensive utilization of stored procedures, and orchestrated activities like metadata, lookup, set variable, and notably, the versatile Databricks activity. Inherent in its design is the capability to harness structures such as the 'ForEach' loop and conditional 'If' operations. Furthermore, post data integration operations like 'Delete Activity' and 'Execute Pipeline Activity' cement its commitment to both efficiency and compliance.
                        </p>
<p>
                            This documentation offers a deep dive into each pipeline's anatomy, casting light on the nuanced processes and architectural prowess that form the bedrock of this project's advanced data integration methodology. Welcome to an exploration that unveils the confluence of best practices and technology in ETL.
                        </p>
</section>
<div class="image-container">
<h2>A Data Minimization pipeline 1 Overview </h2>
<h4>Pipeline 1, which takes data from the on-prem database using integration runtime.</h4></div>
<div class="image-container">
<div class="img-box" style="flex: 1; margin: 0 5px;">
<img alt="ETL Pipeline 1" data-aos="zoom-in-down" src="assets/pipeline 1.png" style="max-width: 100%; height: auto;"/>
</div>
<div class="img-box" style="flex: 1; margin: 0 5px;">
<img alt="ETL Pipeline 2" data-aos="zoom-in-down" src="assets/pipeline 1 If condition.png" style="max-width: 100%; height: auto;"/>
</div>
</div></div>
<h2>Data Minimization And Metadata collection in pipeline 1</h2>
<div class="img-box" style="flex: 1; margin: 0 5px; display: flex; justify-content: center; align-items: center;">
<video autoplay="" controls="" loop="" style="max-width: 100%; max-height: 100%; width: auto; height: auto;">
<source src="assets/Data Minimization Pipeline1.mp4" type="video/mp4"/>
    Your browser does not support the video tag.
  </video>
</div>
<p class="true-branch">
<br/><br/>
  In the above video, I have explained the Data minimization procedure which incoprates with the UK GFPR Check <a href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-protection-principles/a-guide-to-the-data-protection-principles/the-principles/data-minimisation/#data_minimisation"><span style="color: rgb(0, 130, 236);">Article 5(1)(c) </span></a>, which says 
  we supposed to only processed the data which is "adequate, relevant and limited to what is necessary" for the relevent the the purpose.
  In our case we have processed not only specific tables as well as the specific columns in the PII data such as Customer data. Also performed the
  pseudonymization on that seleted minimized version of the customer table and the minimization of Table columns has conducted in on-prem DB where we have done pseudonymization And above process
  we are takeing the relevent tables only, we had other PII data also like EMployee Data which we did not need so we did not take it into our ETL procedure.
</p>
<div class="container">
<p>In <strong>Pipeline 1</strong>, our core objective revolves around extracting data from an on-premises database and seamlessly staging it within Azure Data Lake. The process initiates with a <em>'Lookup on-prem'</em>, meticulously designed to establish connectivity with the on-premises database. Within this sphere, a specialized stored procedure is executed, which is geared towards retrieving both the schema and corresponding table details of the on-premises database.</p>
<p>Subsequently, the pipeline employs a <em>'For Each table'</em> to process the data derived from the Lookup Activity. Within this loop, two primary activities are conducted:</p>
<ol>
<li><strong>Stored Procedure</strong>:<p>  This procedure <em>'on-prem metadata'</em>ingests the schema and table name harvested from the Lookup Activity output. It subsequently populates the metadata into a designated SQL Server table. Notably, the metadata encompasses the schema name, table name, pipeline ID, and the exact timestamp of the pipeline execution.</p></li>
<li><strong>Copy Data Activity</strong>:<p>  This activity <em>'Stagging Area tables'</em>establishes a link with the on-premises database and, using the aforementioned schema and table name details, fetches the corresponding files.</p></li>
</ol>
<p>Conclusively, the extracted tables find their repository within a staging area container, aptly named <em>'raw-data-container'</em>.</p>
</div>
<h2>A Data Classification Pipeline 2 Overview</h2>
<p>In <strong>Pipeline 2</strong>, the process commences with an activity termed <em>'Stagging Metadata'</em>, tailored to fetch child items, consequently producing an output of table names. 
 This is followed by a <em>'Foreach'</em> activity, set up to loop through each derived item based on the metadata's output value. Nestled within this loop is a conditional check:</p><br/>
<div class="condition-box">
<p><code>@Containes(item().name, 'dbo.Customers.txt')</code></p>
<div class="true-branch">
<strong>If True</strong>: 
                                    <p>A <em>'Copy Data Activity'</em> triggers, transferring the 'Customers.txt' and 'Employees.txt' tables into a designated <em>'PII container'</em> in blob storage. This is in compliance with GDPR's principles, particularly concerning the processing of PII data, hence isolating them in a distinct container.</p>
</div>
<div class="false-branch">
<strong>If False</strong>: 
                                    <p>The corresponding <em>'Copy Data Activity'</em> initiates, redirecting files into another container labeled <em>'Business-data-container'</em>. Any file not adhering to the aforementioned condition finds its residence here.</p>
</div>
</div>
<p class="true-branch">Upon successful copying, a subsequent <em>'Delete Activity'</em> ensues, interconnected with both true and false segments of the copy activity. This action purges tables from the data lake post-copy, epitomizing the GDPR's mandates on <strong>Data Minimization and Retention</strong>, thereby ensuring data is not retained beyond its necessary lifespan and only essential data is processed.</p>
<p class="true-branch"><em>Note</em>: Adherence to GDPR's data principles ensures that businesses respect the privacy rights of individuals while handling and processing their data, reinforcing trust and maintaining organizational integrity.</p>
<div class="image-container">
<div class="img-box" style="flex: 1; margin: 0 5px;">
<img alt="ETL Pipeline 2" data-aos="zoom-in-down" src="assets/P2 inside.png" style="max-width: 100%; height: auto;"/>
</div>
<div class="img-box" style="flex: 1; margin: 0 5px;">
<img alt="ETL Pipeline 1" data-aos="zoom-in-down" src="assets/P2.png" style="max-width: 100%; height: auto;"/>
</div>
</div>
<div class="true-branch">
<h4>Data Classification in pipeline 2</h4>
<p>In the video, I expound upon the procedural intricacies of data classification. The pivotal <em>ForEach Loop</em> is harnessed to meticulously traverse each file name within the raw-data container housed in the ADLS repository.</p>
<p>At the crux of this discourse lies the strategic application of the <em>ForEach Loop</em>, facilitating the systematic categorization of data files. This process operates in strict accordance with <strong>UK General Data Protection Regulation (GDPR)</strong> guidelines.</p>
<p>A significant criterion governs this classification paradigm. Files containing sensitive customer data, paramount under the <strong>UK GDPR</strong>, undergo a bespoke procession. These files are discerningly relocated to a designated enclave within the <em>PII (Personally Identifiable Information)</em> container, thus upholding stringent data protection mandates.</p>
<p>Conversely, files of non-sensitive nature are channeled seamlessly into the <em>Business-data container</em>. This bifurcated approach signifies a harmonious balance between the technical efficacy of the <em>ForEach Loop</em> and the dictates of the <strong>UK GDPR</strong>.</p>
<p>The narrative coalescing within the video presentation epitomizes a harmonized blend of procedural finesse and legal imperative. It intertwines the meticulous data management enabled by the <em>ForEach Loop</em> with the stipulations of the <strong>UK GDPR</strong>, underscoring a commitment to data integrity and privacy within a complex digital landscape.</p>
</div>
<div class="img-box" style="flex: 1; margin: 0 5px; display: flex; justify-content: center; align-items: center;">
<video autoplay="" controls="" loop="" style="max-width: 100%; max-height: 100%; width: auto; height: auto;">
<source src="assets/Pipeline 2 Data Classification only.mp4" type="video/mp4"/>
    Your browser does not support the video tag.
  </video>
</div>
<div class="true-branch">
<h4>Metadata table creation and store procedure</h4>
<p>The video below provides a detailed explanation of the process behind creating metadata collection tables and store procedures. In this informative video, we're focusing on how these important elements are set up right after we've completed the data classification phase within both "Copy Data" activities.</p>
<p>In simpler terms, we'll show you how we've built tables to collect and organize essential details about the data classification process. These tables help us keep track of important information, like where the data comes from, the results of the classification, and when these actions occurred. This gives us a clearer picture of how our data is being categorized and processed.</p>
<p>Furthermore, we'll guide you through the process of integrating store procedures after the data classification step. Think of store procedures as sets of instructions that are executed automatically. They come into play within the "Copy Data" activities, using the classification results to streamline processes. This integration enables us to take immediate actions based on how the data was classified.</p>
<p>To sum it up, this video highlights the significant roles that metadata collection tables and store procedures play in enhancing our data management practices. By demonstrating how these components work together following data classification, we aim to provide you with a clearer understanding of how they improve data processing and decision-making tasks.</p>
<div class="img-box" style="flex: 1; margin: 0 5px; display: flex; justify-content: center; align-items: center;">
<video autoplay="" controls="" loop="" style="max-width: 100%; max-height: 100%; width: auto; height: auto;">
<source src="assets/Pipeline 2 Metadata After Data Classification.mp4" type="video/mp4"/>
    Your browser does not support the video tag.
  </video>
</div>
</div>
<h4>Metadata Collection After Data classifications</h4>
<div class="image-container">
<div class="img-box" style="flex: 1; margin: 0 5px;">
<img alt="ETL Pipeline 1" data-aos="zoom-in-down" src="assets/Pipeline 2 DB Metadata After Classification Documenting.png" style="max-width: 100%; height: auto;"/>
</div>
</div>
<h2>Data Transformation Pipeline 3 Overview</h2>
<h4>Constructing Fact and Dimension Tables in Data Warehousing</h4>
<p class="true-branch">
    Fact and dimension tables form the backbone of a star schema in a data warehouse. Here, we will walk through the detailed process of constructing these tables using the Northwind database as an example.
</p>
<p class="true-branch">
    The Customer database provides a series of tables that capture business transactions and products. By transforming this data, we can create a star schema which simplifies querying and enhances performance.
</p>
<h2>Step-by-Step Process</h2>
<ol>
<li>
<strong>Understanding the Data:</strong> Begin by analyzing the structure and relationships in the Northwind database. This helps determine which tables serve as facts and dimensions.
    </li>
<li>
<strong>Creating the Fact Table:</strong> We used a series of merges to combine relevant tables into our primary fact table, named 'Order_fact'. This table captures the essence of business transactions.
    </li>
<li>
<strong>Constructing Dimension Tables:</strong> Dimension tables such as `ShipperDim`, `DateDim`, `Product_dim`, and `Customer_dim` were created to provide descriptive, textual, or categorical information about the facts.
    </li>
<li>
<strong>Establishing Relationships:</strong> We employed surrogate keys to connect the fact table with dimension tables, ensuring a streamlined one-to-many relationship.
    </li>
<li>
<strong>Data Quality Checks:</strong> Before finalizing, we conducted thorough data quality checks to ensure there were no null values and that relationships were established correctly.
    </li>
</ol>
<!-- Placeholder for video -->
<div class="img-box">
<h4 style="text-align: center;">Data after Fact and Dimensions tables.</h4>
<img alt="ETL Pipeline 3" data-aos="zoom-in-down" src="assets\P3 Notebook and Foreach.png"/>
</div>
<p class="true-branch">
    Constructing fact and dimension tables is a critical step in data warehousing, allowing for efficient querying and in-depth analytics. By following the steps outlined above, one can effectively transform raw business data into a structured star schema.
</p>
<div class="image-container">
<h4>Store Procedure After Databricks Transformation</h4>
<div class="img-box">
<img alt="ETL Pipeline 1" data-aos="zoom-in-down" src="assets\P3 Store procedure values.png"/>
</div>
<p>In <strong>Pipeline 3</strong>,The process begins with an activity called <em style="color: #FF2F5D;">'Azure Databricks Notebook'</em>. Using the service principal's create key, we gain access to the blob storage
    In the transformation process for Pipeline 3, a series of steps were meticulously executed to derive fact and dimension tables from the Customers database.</p>
<h2>Transformation Steps</h2>
<ol class="ss">
<li><strong> <p>Library Imports:</p></strong> Necessary libraries for data analysis and database connectivity were imported.</li></ol></div>
<li><strong> <p>Blob Storage Data Extraction:</p></strong> Data was sourced from the blob storage, specifically from containers named 'PII-container' and 'Business-data-container'.</li>
<li><strong> <p>Data Cleaning:</p></strong>In some column had the null data so all data cleaning with respected to the business has been done through the databricks.</li>
<li><strong> <p>Fact Table Creation:</p></strong> Using Python libraries like Pandas and Numpy I have processed the data for fact and dimension tables.</li>
<li><strong> <p>Fact and Dimension Table Formulation:</p></strong> Various tables were developed using the fact-dimension data modeling approach.</li>
<li><strong> <p>Databricks Notebook &amp; Array Documentation:</p></strong> All transformations were documented and passed to the next Azure Data Factory activity.</li>
<li><strong> <p>Storing Transformation Metadata:</p></strong> Metadata was collected and stored in the "Transformation Metadata" table.</li>
<li><strong> <p> Data Deletion and Compliance:</p></strong> Post data storage, deletion activities ensured alignment with UK GDPR policies.</li>
<p>In the below link you can find the transformation which I have done on the tables and made the fact and dimensions tables also the Data Quality Check logics and metadata documentation and dumping the fact and dimensions tables into Azure SQL Server.</p>
<a href="Northwind Transformation.html" style="color: rgb(0, 95, 136);">Check the Databricks Transformation code and DQC logic</a>
<div class="headd">
<h4>Metadata collection table creation and store procedure for Pipeline 3</h4>
</div>
<div class="img-box" style="flex: 1; margin: 0 5px; display: flex; justify-content: center; align-items: center;">
<video autoplay="" controls="" loop="" style="max-width: 100%; max-height: 100%; width: auto; height: auto;">
<source src="assets/Pipeline 3 After Transformation Complected Video.mp4" type="video/mp4"/>
    Your browser does not support the video tag.
  </video>
</div>
<h2 class="true-branch">All pipelines Run</h2>
<h4 class="true-branch">all pipelines are connected to each other and work after each pipeline complection see below video.</h4>
<p class="true-branch">In our comprehensive data management workflow, we've designed a strategic sequence of four pipelines, each serving a specific role in our data processing journey.</p>
<p class="true-branch">The first step is the <b>Data Extraction and Minimization Pipeline</b>. Here, data is meticulously extracted from various sources, followed by a crucial minimization process to eliminate redundancy and irrelevant information. This ensures a streamlined dataset as a foundation for the subsequent stages.</p>
<p class="true-branch">Next comes the <b>Data Classification Pipeline</b>, where sophisticated algorithms are employed to categorize data based on predefined criteria. This meticulous classification process ensures that data aligns with regulatory and organizational guidelines, enhancing data quality and compliance.</p>
<p class="true-branch">Proceeding to the <b>Data Transformation Pipeline</b>, we refine the extracted and classified data. This involves data enrichment, consolidation, and standardization, optimizing data for analysis and further use in downstream processes.</p>
<p class="true-branch">The pinnacle of our orchestrated workflow is the <b>Execute Pipeline Activity</b>, our fourth pipeline. This central component orchestrates the seamless execution of the preceding three pipelines. Through this mechanism, data extraction, classification, and transformation are harmoniously orchestrated to unfold as a cohesive workflow.</p>
<p class="true-branch">This intricate data orchestration process ensures that your data follows a systematic journey, from extraction and classification to transformation and orchestrated execution. The combined strength of these pipelines empowers you to leverage your data's full potential while adhering to best practices, regulatory standards, and tailored data processing needs.</p>
<div class="img-box" style="flex: 1; margin: 0 5px; display: flex; justify-content: center; align-items: center;">
<video autoplay="" controls="" loop="" style="max-width: 100%; max-height: 100%; width: auto; height: auto;">
<source src="assets/All pipeline Run Pipeline 4.mp4" type="video/mp4"/>
    Your browser does not support the video tag.
  </video>
</div>
<br/><br/><br/> <br/><br/><br/>
<p class="true-branch">In the realm of data management, our orchestrated ETL processes stand as a testament to the power of structured workflows. The seamless integration of our Data Extraction and Minimization Pipeline, Data Classification Pipeline, Data Transformation Pipeline, and the central Execute Pipeline Activity has revolutionized the way we handle data. These interconnected pipelines not only streamline the journey of data from its source to its refined and orchestrated execution but also ensure compliance with regulations and best practices.</p>
<p class="true-branch">At the heart of our data management philosophy lie the principles of the UK General Data Protection Regulation (GDPR). The meticulous implementation of policies such as data minimization, which involves extracting only the essential data for processing, ensures that our datasets are lean, efficient, and focused. Additionally, our adherence to data retention policies guarantees that information is stored for the appropriate period, aligning with legal and operational requirements.</p>
<p class="true-branch">Moreover, pseudonymization plays a pivotal role in protecting individual privacy. By replacing identifiable attributes with pseudonyms, we strike a balance between data utility and confidentiality. This strategic approach not only facilitates seamless data processing but also safeguards sensitive information, fortifying our commitment to ethical and secure data practices.</p>
<p class="true-branch">Through this strategic orchestration, we empower our data to unveil its full potential, driving informed decisions and fostering data-driven excellence while prioritizing the protection of individual rights and data integrity.</p>
<h2 class="headd">Final Dashboard using Powerbi</h2>
<div class="image-container">
<h4>After accessing pseudonymized data from Azure SQL server below is the Sales analysis Dashboard which is considered as a ethical processed
    data extraction, trasnformation according to the followed procedure as per UK GDPR defined terms. </h4>
<div class="img-box">
<img alt="ETL Pipeline 1" data-aos="zoom-in-down" src="assets\dash.png"/>
</div>
<p>The dashboard showcases essential fields, including the PseudoCustID, PseudoCompName, PseudoContactName, PseudoContactTitle, City, and Country. These data points are strategically pseudonymized, replacing identifiable attributes with pseudonyms to strike a harmonious balance between data utility and individual privacy. This transformation safeguards sensitive customer information while enabling insightful analysis.</p>
<p>Incorporating UK GDPR principles, our dashboard embodies the spirit of data protection and ethical data practices. By showcasing transformed PII customer data, we affirm our dedication to safeguarding individual rights and ensuring transparent, secure data utilization. As you explore this dynamic dashboard, we invite you to appreciate the intricate dance between data insights and responsible data stewardship that shapes our data-driven endeavors.</p>
<br/><br/><br/> <br/><br/><br/>
<!-- Statistics -->
<!-- Footer -->
<footer>
<div class="container-fluid">
<div class="row footer">
<div id="about">
<h1>Siddharth<br/><span>Chikalkar</span></h1>
<p>I am currently pursuing an MSc in Data Analytics at Aston University in Birmingham. I hold a Bachelor's degree in Computer Application (BCA) from India, where I achieved a GPA of 8.68. With a strong foundation in data analytics and computer science, I have developed a diverse skill set. I am experienced in utilizing tools such as Power BI, SQL, and Excel for data management, visualization, and analysis. Through various projects, I have gained expertise in data cleaning, transformation, and developing insightful dashboards. Additionally, I have worked on machine learning projects, applying algorithms for loan approval prediction and facial expression detection. My passion for leveraging data to drive informed decision-making is matched by my proficiency in Python, Django, and TensorFlow. With a keen eye for detail and a drive for continuous learning, I am dedicated to delivering accurate and impactful data-driven solutions.</p>
<ul class="social-links">
<li><a href="https://www.linkedin.com/in/siddharth-chikalkar-7244141b0"><img src="assets/behance.png"/></a></li>
</ul>
</div>
</div>
</div>
</footer>
<!-- Some Javascript -->
<script src="js/jquery-2.1.1.js"></script>
<script src="js/swiper.jquery.min.js"></script>
<!-- Initialize Client Swiper -->
<script>
      var swiper1 = new Swiper('.client-swiper', {
        slidesPerView: 3,
        paginationClickable: true,
        nextButton: '.swiper-button-next',
        prevButton: '.swiper-button-prev',
        spaceBetween: 60,
        // Responsive breakpoints
        breakpoints: {
          // when window width is <= 320px
          320: {
            slidesPerView: 1,
            spaceBetween: 10,
            pagination: '.swiper-pagination'
          },
          // when window width is <= 480px
          480: {
            slidesPerView: 1,
            spaceBetween: 20
          },
          // when window width is <= 640px
          640: {
            slidesPerView: 1,
            spaceBetween: 30
          }
        }
      });
      // Initialize Testimonial Swiper
      var swiper2 = new Swiper('.testimonial-swiper', {
        slidesPerView: 3,
        pagination: '.swiper-pagination',
        paginationClickable: true,
        spaceBetween: 30,
        grabCursor: true,
        freeMode: true,
        breakpoints: {
          // when window width is <= 320px
          320: {
            slidesPerView: 1,
            spaceBetween: 10,
          },
          // when window width is <= 480px
          480: {
            slidesPerView: 1,
            spaceBetween: 10
          },
          // when window width is <= 640px
          640: {
            slidesPerView: 1,
            spaceBetween: 10
          }
        }
      });
      </script>
<script src="http://cdnjs.cloudflare.com/ajax/libs/waypoints/2.0.3/waypoints.min.js"></script>
<script src="js/jquery.counterup.min.js"></script>
<script>
      // Counterup
      $('.counter').counterUp({
        time: 1000
      });

      // Main Navigation
      $('#menu-toggle').click(function(){
        $(this).toggleClass('open'),
        $('.main-nav').toggleClass('show-it');
      })
      </script>
<!-- Google Analytics - You should remove this -->
<script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-29231762-2', 'auto');
      ga('send', 'pageview');



     
        AOS.init();
     
      </script>
</div>